"""Time series models for hazard prediction."""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import logging
import warnings
warnings.filterwarnings('ignore')

# Deep Learning
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Time Series
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
try:
    from prophet import Prophet
except ImportError:
    Prophet = None

# Sklearn
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import MinMaxScaler
import joblib

from config import config

logger = logging.getLogger(__name__)

class TimeSeriesModels:
    """Time series models for hazard prediction."""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.model_configs = config.models
        self.trained_models = {}
        
    def prepare_lstm_data(self, data: pd.DataFrame, target_column: str,
                         sequence_length: int = 24, test_size: float = 0.2) -> Tuple:
        """Prepare data for LSTM model."""
        
        # Ensure data is sorted by time
        data = data.sort_index()
        
        # Scale the data
        scaler = MinMaxScaler()
        scaled_data = scaler.fit_transform(data[[target_column]])
        self.scalers[target_column] = scaler
        
        # Create sequences
        X, y = [], []
        for i in range(sequence_length, len(scaled_data)):
            X.append(scaled_data[i-sequence_length:i, 0])
            y.append(scaled_data[i, 0])
        
        X, y = np.array(X), np.array(y)
        
        # Reshape for LSTM [samples, time steps, features]
        X = X.reshape((X.shape[0], X.shape[1], 1))
        
        # Split into train/test
        split_idx = int(len(X) * (1 - test_size))
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        return X_train, X_test, y_train, y_test
    
    def build_lstm_model(self, sequence_length: int = 24, features: int = 1) -> Sequential:
        """Build LSTM model architecture."""
        
        lstm_config = self.model_configs.get('lstm', {})
        
        model = Sequential([
            LSTM(lstm_config.get('hidden_units', 128), 
                 return_sequences=True, 
                 input_shape=(sequence_length, features)),
            Dropout(0.2),
            LSTM(lstm_config.get('hidden_units', 128) // 2, 
                 return_sequences=False),
            Dropout(0.2),
            Dense(50, activation='relu'),
            Dense(1, activation='sigmoid')
        ])\n        \n        model.compile(\n            optimizer=Adam(learning_rate=0.001),\n            loss='mse',\n            metrics=['mae']\n        )\n        \n        return model\n    \n    def train_lstm_model(self, data: pd.DataFrame, target_column: str,\n                        sequence_length: int = None) -> Dict:\n        \"\"\"Train LSTM model for time series prediction.\"\"\"\n        \n        logger.info(f\"Training LSTM model for {target_column}\")\n        \n        if sequence_length is None:\n            sequence_length = self.model_configs.get('lstm', {}).get('sequence_length', 24)\n        \n        # Prepare data\n        X_train, X_test, y_train, y_test = self.prepare_lstm_data(\n            data, target_column, sequence_length\n        )\n        \n        # Build model\n        model = self.build_lstm_model(sequence_length)\n        \n        # Callbacks\n        callbacks = [\n            EarlyStopping(patience=10, restore_best_weights=True),\n            ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-7)\n        ]\n        \n        # Train model\n        lstm_config = self.model_configs.get('lstm', {})\n        history = model.fit(\n            X_train, y_train,\n            epochs=lstm_config.get('epochs', 100),\n            batch_size=lstm_config.get('batch_size', 32),\n            validation_data=(X_test, y_test),\n            callbacks=callbacks,\n            verbose=0\n        )\n        \n        # Evaluate model\n        train_pred = model.predict(X_train)\n        test_pred = model.predict(X_test)\n        \n        # Inverse transform predictions\n        scaler = self.scalers[target_column]\n        train_pred_inv = scaler.inverse_transform(train_pred)\n        test_pred_inv = scaler.inverse_transform(test_pred)\n        y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))\n        y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n        \n        # Calculate metrics\n        train_rmse = np.sqrt(mean_squared_error(y_train_inv, train_pred_inv))\n        test_rmse = np.sqrt(mean_squared_error(y_test_inv, test_pred_inv))\n        train_mae = mean_absolute_error(y_train_inv, train_pred_inv)\n        test_mae = mean_absolute_error(y_test_inv, test_pred_inv)\n        \n        # Store model\n        model_key = f'lstm_{target_column}'\n        self.trained_models[model_key] = {\n            'model': model,\n            'scaler': scaler,\n            'sequence_length': sequence_length,\n            'target_column': target_column\n        }\n        \n        results = {\n            'model_type': 'LSTM',\n            'target_column': target_column,\n            'train_rmse': train_rmse,\n            'test_rmse': test_rmse,\n            'train_mae': train_mae,\n            'test_mae': test_mae,\n            'model_key': model_key,\n            'history': history.history\n        }\n        \n        logger.info(f\"LSTM model trained. Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}\")\n        \n        return results\n    \n    def build_cnn_lstm_model(self, sequence_length: int = 24, features: int = 1) -> Model:\n        \"\"\"Build CNN-LSTM hybrid model.\"\"\"\n        \n        inputs = Input(shape=(sequence_length, features))\n        \n        # CNN layers for feature extraction\n        x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n        x = MaxPooling1D(pool_size=2)(x)\n        x = Conv1D(filters=32, kernel_size=3, activation='relu')(x)\n        x = MaxPooling1D(pool_size=2)(x)\n        \n        # LSTM layers for temporal modeling\n        x = LSTM(50, return_sequences=True)(x)\n        x = Dropout(0.2)(x)\n        x = LSTM(25)(x)\n        x = Dropout(0.2)(x)\n        \n        # Dense layers\n        x = Dense(25, activation='relu')(x)\n        outputs = Dense(1, activation='sigmoid')(x)\n        \n        model = Model(inputs=inputs, outputs=outputs)\n        model.compile(\n            optimizer=Adam(learning_rate=0.001),\n            loss='mse',\n            metrics=['mae']\n        )\n        \n        return model\n    \n    def train_arima_model(self, data: pd.Series, order: Tuple[int, int, int] = (2, 1, 2)) -> Dict:\n        \"\"\"Train ARIMA model for time series prediction.\"\"\"\n        \n        logger.info(f\"Training ARIMA model with order {order}\")\n        \n        try:\n            # Prepare data\n            data_clean = data.dropna()\n            \n            # Split data\n            split_idx = int(len(data_clean) * 0.8)\n            train_data = data_clean[:split_idx]\n            test_data = data_clean[split_idx:]\n            \n            # Fit ARIMA model\n            model = ARIMA(train_data, order=order)\n            fitted_model = model.fit()\n            \n            # Make predictions\n            train_pred = fitted_model.fittedvalues\n            test_pred = fitted_model.forecast(steps=len(test_data))\n            \n            # Calculate metrics\n            train_rmse = np.sqrt(mean_squared_error(train_data[1:], train_pred[1:]))\n            test_rmse = np.sqrt(mean_squared_error(test_data, test_pred))\n            train_mae = mean_absolute_error(train_data[1:], train_pred[1:])\n            test_mae = mean_absolute_error(test_data, test_pred)\n            \n            # Store model\n            model_key = f'arima_{data.name}'\n            self.trained_models[model_key] = {\n                'model': fitted_model,\n                'order': order,\n                'target_column': data.name\n            }\n            \n            results = {\n                'model_type': 'ARIMA',\n                'target_column': data.name,\n                'order': order,\n                'train_rmse': train_rmse,\n                'test_rmse': test_rmse,\n                'train_mae': train_mae,\n                'test_mae': test_mae,\n                'model_key': model_key,\n                'aic': fitted_model.aic,\n                'bic': fitted_model.bic\n            }\n            \n            logger.info(f\"ARIMA model trained. Test RMSE: {test_rmse:.4f}, AIC: {fitted_model.aic:.2f}\")\n            \n            return results\n            \n        except Exception as e:\n            logger.error(f\"Error training ARIMA model: {e}\")\n            return {'error': str(e)}\n    \n    def train_sarima_model(self, data: pd.Series, \n                          order: Tuple[int, int, int] = (1, 1, 1),\n                          seasonal_order: Tuple[int, int, int, int] = (1, 1, 1, 24)) -> Dict:\n        \"\"\"Train SARIMA model for seasonal time series.\"\"\"\n        \n        logger.info(f\"Training SARIMA model with order {order}, seasonal {seasonal_order}\")\n        \n        try:\n            data_clean = data.dropna()\n            \n            # Split data\n            split_idx = int(len(data_clean) * 0.8)\n            train_data = data_clean[:split_idx]\n            test_data = data_clean[split_idx:]\n            \n            # Fit SARIMA model\n            model = SARIMAX(\n                train_data, \n                order=order, \n                seasonal_order=seasonal_order\n            )\n            fitted_model = model.fit(disp=False)\n            \n            # Make predictions\n            train_pred = fitted_model.fittedvalues\n            test_pred = fitted_model.forecast(steps=len(test_data))\n            \n            # Calculate metrics\n            train_rmse = np.sqrt(mean_squared_error(train_data, train_pred))\n            test_rmse = np.sqrt(mean_squared_error(test_data, test_pred))\n            train_mae = mean_absolute_error(train_data, train_pred)\n            test_mae = mean_absolute_error(test_data, test_pred)\n            \n            # Store model\n            model_key = f'sarima_{data.name}'\n            self.trained_models[model_key] = {\n                'model': fitted_model,\n                'order': order,\n                'seasonal_order': seasonal_order,\n                'target_column': data.name\n            }\n            \n            results = {\n                'model_type': 'SARIMA',\n                'target_column': data.name,\n                'order': order,\n                'seasonal_order': seasonal_order,\n                'train_rmse': train_rmse,\n                'test_rmse': test_rmse,\n                'train_mae': train_mae,\n                'test_mae': test_mae,\n                'model_key': model_key,\n                'aic': fitted_model.aic,\n                'bic': fitted_model.bic\n            }\n            \n            logger.info(f\"SARIMA model trained. Test RMSE: {test_rmse:.4f}, AIC: {fitted_model.aic:.2f}\")\n            \n            return results\n            \n        except Exception as e:\n            logger.error(f\"Error training SARIMA model: {e}\")\n            return {'error': str(e)}\n    \n    def train_prophet_model(self, data: pd.DataFrame, date_column: str, \n                           value_column: str) -> Dict:\n        \"\"\"Train Prophet model for time series forecasting.\"\"\"\n        \n        if Prophet is None:\n            logger.error(\"Prophet not available. Install with: pip install prophet\")\n            return {'error': 'Prophet not installed'}\n        \n        logger.info(f\"Training Prophet model for {value_column}\")\n        \n        try:\n            # Prepare data for Prophet\n            prophet_data = data[[date_column, value_column]].copy()\n            prophet_data.columns = ['ds', 'y']\n            prophet_data = prophet_data.dropna()\n            \n            # Split data\n            split_idx = int(len(prophet_data) * 0.8)\n            train_data = prophet_data[:split_idx]\n            test_data = prophet_data[split_idx:]\n            \n            # Initialize and fit Prophet model\n            model = Prophet(\n                daily_seasonality=True,\n                weekly_seasonality=True,\n                yearly_seasonality=True,\n                changepoint_prior_scale=0.05,\n                seasonality_mode='multiplicative'\n            )\n            \n            # Add custom seasonalities for ocean data\n            model.add_seasonality(\n                name='tidal', \n                period=0.5,  # ~12 hours (tidal cycle)\n                fourier_order=3\n            )\n            \n            model.fit(train_data)\n            \n            # Make predictions\n            future = model.make_future_dataframe(periods=len(test_data), freq='H')\n            forecast = model.predict(future)\n            \n            # Extract predictions\n            train_pred = forecast['yhat'][:len(train_data)]\n            test_pred = forecast['yhat'][len(train_data):]\n            \n            # Calculate metrics\n            train_rmse = np.sqrt(mean_squared_error(train_data['y'], train_pred))\n            test_rmse = np.sqrt(mean_squared_error(test_data['y'], test_pred))\n            train_mae = mean_absolute_error(train_data['y'], train_pred)\n            test_mae = mean_absolute_error(test_data['y'], test_pred)\n            \n            # Store model\n            model_key = f'prophet_{value_column}'\n            self.trained_models[model_key] = {\n                'model': model,\n                'target_column': value_column,\n                'date_column': date_column\n            }\n            \n            results = {\n                'model_type': 'Prophet',\n                'target_column': value_column,\n                'train_rmse': train_rmse,\n                'test_rmse': test_rmse,\n                'train_mae': train_mae,\n                'test_mae': test_mae,\n                'model_key': model_key,\n                'forecast': forecast\n            }\n            \n            logger.info(f\"Prophet model trained. Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}\")\n            \n            return results\n            \n        except Exception as e:\n            logger.error(f\"Error training Prophet model: {e}\")\n            return {'error': str(e)}\n    \n    def predict_future(self, model_key: str, steps: int = 24) -> Dict:\n        \"\"\"Make future predictions using trained model.\"\"\"\n        \n        if model_key not in self.trained_models:\n            raise ValueError(f\"Model {model_key} not found. Train the model first.\")\n        \n        model_info = self.trained_models[model_key]\n        model_type = model_key.split('_')[0]\n        \n        try:\n            if model_type == 'lstm':\n                # LSTM prediction logic would go here\n                # This is a simplified version\n                predictions = np.random.normal(0.5, 0.1, steps)  # Placeholder\n                \n            elif model_type in ['arima', 'sarima']:\n                model = model_info['model']\n                predictions = model.forecast(steps=steps)\n                \n            elif model_type == 'prophet':\n                model = model_info['model']\n                future = model.make_future_dataframe(periods=steps, freq='H')\n                forecast = model.predict(future)\n                predictions = forecast['yhat'][-steps:].values\n            \n            else:\n                raise ValueError(f\"Unknown model type: {model_type}\")\n            \n            return {\n                'model_key': model_key,\n                'predictions': predictions.tolist() if hasattr(predictions, 'tolist') else predictions,\n                'steps': steps,\n                'model_type': model_type\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error making predictions with {model_key}: {e}\")\n            return {'error': str(e)}\n    \n    def evaluate_model(self, model_key: str, test_data: pd.Series) -> Dict:\n        \"\"\"Evaluate trained model on test data.\"\"\"\n        \n        if model_key not in self.trained_models:\n            raise ValueError(f\"Model {model_key} not found\")\n        \n        # Make predictions\n        predictions = self.predict_future(model_key, len(test_data))\n        \n        if 'error' in predictions:\n            return predictions\n        \n        pred_values = np.array(predictions['predictions'])\n        \n        # Calculate metrics\n        rmse = np.sqrt(mean_squared_error(test_data, pred_values))\n        mae = mean_absolute_error(test_data, pred_values)\n        r2 = r2_score(test_data, pred_values)\n        \n        # Calculate directional accuracy\n        direction_actual = np.diff(test_data) > 0\n        direction_pred = np.diff(pred_values) > 0\n        directional_accuracy = np.mean(direction_actual == direction_pred)\n        \n        return {\n            'model_key': model_key,\n            'rmse': rmse,\n            'mae': mae,\n            'r2_score': r2,\n            'directional_accuracy': directional_accuracy,\n            'predictions': pred_values.tolist()\n        }\n    \n    def save_models(self, filepath_prefix: str):\n        \"\"\"Save all trained models.\"\"\"\n        \n        for model_key, model_info in self.trained_models.items():\n            filepath = f\"{filepath_prefix}_{model_key}.pkl\"\n            \n            try:\n                if 'model' in model_info and hasattr(model_info['model'], 'save'):\n                    # TensorFlow/Keras models\n                    model_info['model'].save(filepath.replace('.pkl', '.h5'))\n                    # Save additional info\n                    model_data = {k: v for k, v in model_info.items() if k != 'model'}\n                    joblib.dump(model_data, filepath)\n                else:\n                    # Other models\n                    joblib.dump(model_info, filepath)\n                \n                logger.info(f\"Saved model {model_key} to {filepath}\")\n                \n            except Exception as e:\n                logger.error(f\"Error saving model {model_key}: {e}\")\n    \n    def load_models(self, filepath_prefix: str):\n        \"\"\"Load trained models from disk.\"\"\"\n        \n        import os\n        import glob\n        \n        # Find all model files\n        pattern = f\"{filepath_prefix}_*.pkl\"\n        model_files = glob.glob(pattern)\n        \n        for filepath in model_files:\n            try:\n                model_key = os.path.basename(filepath).replace(f\"{os.path.basename(filepath_prefix)}_\", \"\").replace(\".pkl\", \"\")\n                \n                # Load model info\n                model_info = joblib.load(filepath)\n                \n                # Check for TensorFlow model\n                h5_filepath = filepath.replace('.pkl', '.h5')\n                if os.path.exists(h5_filepath):\n                    model_info['model'] = tf.keras.models.load_model(h5_filepath)\n                \n                self.trained_models[model_key] = model_info\n                logger.info(f\"Loaded model {model_key} from {filepath}\")\n                \n            except Exception as e:\n                logger.error(f\"Error loading model from {filepath}: {e}\")